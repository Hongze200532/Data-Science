{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2 lab 1: Supervised learning basics in a polynomial fitting example\n",
    "**This lab**: the aim is to practice the basic setup of supervised learning problems (i.e., the train/test/validation data splits, regularisation) by building and evaluating a polynomial fitting example (a form of regression) and to tune their hyperparameters using cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the seed for numpy.random\n",
    "np.random.seed(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset\n",
    "Write code to sample regularly N=100 data points (x,y) from this model: y = sin(2.pi.x), where 0=<x<=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "x =  # your code\n",
    "y =  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, contaminate the outputs of your samples with gaussian noise (0-mean, variance=0.01) using Numpy's ```randn``` command i.e. ```numpy.random.randn(n)```. Visualise your dataset using matplotlib's ```scatter``` plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use __numpy__ to split data into train-validation-test sets, at porportions $p=20\\%$ for testing and $0.5*(100-p)\\%$ for validation and training. There're many other ways. You may also use this method to split _training_ and _testing_ dataset with __sklearn__:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add noise to y\n",
    "\n",
    "# visualise data using scatter plot i.e. noisy y versus x\n",
    "\n",
    "# split data into train, test, validation portions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial fitting\n",
    "Use __numpy__'s ```polyfit``` command to fit polynomials of different orders ($M$ from 0 to 20) to your train data. Show the validation curves using the MSE (mean squared error) loss. Recall, the validation curves include two curves in the same figure, one showing the training loss vs. polynomial orders, and another showing the validation loss vs. polynomial orders. NB: use may find __numpy__'s ```polyval``` function useful during the performance evaluation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see the over/underfitting cases in your validation curves?\n",
    "\n",
    "What's the best polynomial order? Let's do the grid search to find best polynomial order (Hyperparameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation\n",
    "The custom function _mypolyfit_regularised_ is provided for this exercise. Please download the supplmentary file ```supplement_lab_w4.py```, then import the ```mypolyfit_regularised``` function i.e. \n",
    "```python\n",
    "from supplement_lab_w4 import mypolyfit_regularised\n",
    "```\n",
    "The ```mypolyfit_regularised``` function performs polynomial fitting like numpy's ```polyfit``` but with an added $L^2$ regularisation term (read the function's help to familiarise yourself with). Now, fit a polynomial of order 50, but with 20 different regularisation hyperparameters $\\lambda$ ranging logarithmically from 1e-10 to 10. You can use numpy's ```logspace``` command to create this logarithmic range. Show the corresponding validation curves using the MSE loss. \n",
    "\n",
    "NB: you may find the matplotlibâ€™s ```set_xscale('log')``` a useful plotting function when data on x axis scales logarithmically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see the over/underfitting cases in your validation curves? \n",
    "\n",
    "What's the best regularisation hyperparameter Lambda? Let's do the grid search to find best Lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D grid search\n",
    "Now, let's do a 2-dimensional grid search over the joint space of $\\lambda$ and M to find the best pair of hyperparameters. For the grid search, range the polynomial orders M from 0 to 50, and the regularisation parameter $\\lambda$ logarithmically from 1e-10 to 10. Print the values of the best hyperparameters, as well as the corresponding train, test, validation losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ebb9782cc3a86e012ba5f4e65139adac090c9d77ead591281660a8cb2f89711c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
