{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVvKJOs37EXk"
   },
   "source": [
    "# Week5 lab1: Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-068MYHi-QeJ"
   },
   "source": [
    "**This lab**: the aim is to use PCA dimensionality reduction for data compression, data visualisation, and feature selection. During this lab it is recommended to check the sklearn documentation on pca https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "You'll need `olivettifaces.mat` and `titanic.csv` files to work on this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCa4_1oY9yLI"
   },
   "source": [
    "## Exploring data in PCA plots: \"titanic\" dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6LlNSBV9ydl"
   },
   "source": [
    "Install needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1yqxUbpw7EXm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2bH4KgA7EXn"
   },
   "source": [
    "Load the Titanic dataset (titanic.csv) using Pandas' read_csv function. Use scikit-learn's PCA class from the sklearn.decomposition module to visualize the feature data. From this data, extract a binary (0-1) vector y of labels/outcomes indicating a passenger had survived or not. Create a feature matrix X whose columns are Pclass, Sex, Age, Siblings_SpousesAboard, Parents_ChildrenAboard, and Fare features. You may need to convert the Sex variable (currently a string) into a 0-1 numerical variable to include it in the feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2bH4KgA7EXn"
   },
   "source": [
    "Use scikit-learn's PCA class from the sklearn.decomposition module to visualize the feature data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# Perform PCA\n",
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2bH4KgA7EXn"
   },
   "source": [
    "Detecting anomalies, consider these steps:\n",
    "First, it computes the median of the dataset X using the median() method of a Pandas DataFrame. The median represents the middle value in the dataset, separating the lower 50% of values from the upper 50% of values.\n",
    "\n",
    "Next, it computes a threshold value that represents the maximum distance an observation can be from the median before it is considered an anomaly. The threshold is set to 3 times the standard deviation of the data using the std() method of a Pandas DataFrame. The standard deviation represents the amount of variation or dispersion in the dataset.\n",
    "\n",
    "Finally, it identifies the anomalies in the dataset by checking which observations have an absolute difference greater than the threshold value from the median. The where() function of NumPy is used to return the indices of the anomalies in the dataset. Print out these anomaloes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i1LyFIYW7EXn",
    "outputId": "8fade281-c98c-4975-a995-184284db114c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QvMKeNx7EXo"
   },
   "source": [
    "Using the PCA-transformed data, plot the PCA for the rest of the data in blue, and the anomalies in red to see where they are in the plot. You can use Matplotlib's scatter function to create the plot. The x-axis and y-axis should correspond to the first two principal components. Label the plot accordingly to distinguish the anomalous data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "1WbwOxvE7EXo",
    "outputId": "3c0e2f4d-5dca-435f-d86d-95d0ffd2cf1b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIZktnkt9kIJ"
   },
   "source": [
    "## GDimensionality reduction: compression/reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgsqYDw87EXo"
   },
   "source": [
    "Gaussian distribution data: Create a 2D feature matrix (dataset) containing 500 samples from a Gaussian distribution centred at [0,0] with a random covariance matrix. Scatter plot this data. Since covariance is not diagonal, you should be able to see that data will have correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "-2koDCLB7EXo",
    "outputId": "4e0b6600-c992-4a97-9f91-c02a3af3afba",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cL0uEKyH7EXp"
   },
   "source": [
    "Use PCA commend to compute the principal components (PCs) of this data. Plot the PC axes over the 2D dataset's scatter plot. Verify that the two components are orthogonal to each other, and that the first PC indeed shows a direction where data has the most variation (i.e., variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "Uw_wrEms7EXp",
    "outputId": "73535007-c707-4140-c6af-077716521b53",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform PCA on the data\n",
    "# pca = PCA(n_components=2)\n",
    "\n",
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0RcoHVS7EXp"
   },
   "source": [
    "Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance. Use the appropriate output of the pca command to get the data's projections along each of the PC axes. Keep only the projections along the first (most dominant) PC, this is the 1D dimension reduced version of the original 2D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZqyP6Loq7EXp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform the data to get the projections along the PC axes\n",
    "# your code here \n",
    "\n",
    "# Keep only the projections along the first (most dominant) PC\n",
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCUNoEhU7EXp"
   },
   "source": [
    "To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data: Follow the lecture note materials to approximately reconstruct the higher-dimensional 2D data from this 1D projection. This should involve using appropriate outputs from the pca function about e.g., the mean/centre of the dataset, the first PC vector, as well as projected data in the coordinate of the first PC (i.e., scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "MQBR-N497EXp",
    "outputId": "8b0375f6-8cc8-4a58-86f2-813c282773ab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Perform the inverse transform to reconstruct the 2D data\n",
    "# your code here \n",
    "\n",
    "# Plot the original data and the reconstructed data\n",
    "# your code here \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSHa8kjH7EXq"
   },
   "source": [
    "## Visualisation: PCA vs. tSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbJ_UMvC7EXq"
   },
   "source": [
    "Load the Fisher Iris dataset using scikit-learn's load_iris function. Use scikit-learn's PCA class to reduce the feature dimension of this data from 4 to 2. \n",
    "\n",
    "Visualize the dimension-reduced 2D dataset using Matplotlib's scatter function, where data points are colored according to their class (species) labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "lTelORzW7EXq",
    "outputId": "708b0c7a-3208-48c3-bb6b-c0cf26b09f51",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAX-Hnz27EXq"
   },
   "source": [
    "Repeat the previous task using scikit-learn's TSNE class to reduce the dimension of data to 2D using t-distributed Stochastic Neighbour Embedding (t-SNE), and then visualize this dimension-reduced data using Matplotlib's scatter function. \n",
    "\n",
    "Compare the resulting plots from PCA and t-SNE to determine which method gives a clearer insight about the underlying data clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "TMtvBRaS7EXq",
    "outputId": "22174002-5b9c-465c-ccf2-7a491dfb893a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# reduce the dimension of data from 4 to 2 using tSNE\n",
    "# your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNjp51mH7EXq"
   },
   "source": [
    "Download the olivettifaces.mat dataset from https://cs.nyu.edu/~roweis/data.html. The dataset contains grayscale images of faces where pixel values range in [0-255], a few images of several different people, 400 total images (number of samples n=400), 64x64 size (feature size d=4096). Load this data using loadmat function from scipy.io module i.e., loadmat('olivettifaces.mat')\n",
    "\n",
    "Once loaded, access the face data using the 'faces' key in the loaded dictionary. Check the shape of the face data to confirm that it is (4096, 400).\n",
    "\n",
    "Visualize one of the faces from the dataset using Matplotlib's imshow function. Choose a face index from 0 to 399 and reshape the corresponding 4096-dimensional feature vector into a 64x64 image. Plot the image using the cmap='gray' argument to display it in grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "UGxuTHp87EXq",
    "outputId": "c0d7f205-1a26-4cad-f893-0ee20ca7e3f9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.io import loadmat\n",
    "\n",
    "# Load the olivettifaces.mat dataset\n",
    "# data = loadmat('olivettifaces.mat')\n",
    "\n",
    "# Access the face data\n",
    "# faces = data['faces']\n",
    "\n",
    "# Check the shape of the face data\n",
    "# your code here \n",
    "\n",
    "# Example of visualizing one of the faces\n",
    "# your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oz7eu9Op7EXr"
   },
   "source": [
    "Visualise all faces in an image grid (montage). Use numpy functions to reshape and rearrange the face images into a grid. Finally, we visualize the montage using matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "6ECT2-9s7EXr",
    "outputId": "88a87188-53b3-4109-84e3-887b37ed9fc3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESyH8b1F7EXr"
   },
   "source": [
    "Use PCA to compress faces data using the 20 first PCs. Reshape these 20 PC vectors into 20 images and display them. These are known as the Eigen faces, that is all images in the face dataset can be (approximately) described by the linear combination of these 20 PCs. Can you see what facial features each of the Eigen faces do capture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 842
    },
    "id": "zJrzMX3N7EXr",
    "outputId": "1714cef6-53ba-4f8c-fd6e-5067fb08239f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlzgyvvD7EXr"
   },
   "source": [
    "Finally, use appropriate output from the pca command to print how much (%) of the variance (energy) of the original dataset is explained by the first 20 principal components. How many principal components would be needed to have a compression loss less than 5%? NB: The PCA dimensionality reduction is a lossy compression; a measure of PCA's compression loss could be 1 - explained_variance.\n",
    "\n",
    "Hints:\n",
    "- To calculate the explained variance for the first 20 principal components, you can use the explained_variance_ratio_ attribute of the PCA \n",
    "- To determine how many principal components are needed to have a compression loss less than 5%, you can use np.argmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "IIaM_KD07EXr",
    "outputId": "e71a9da4-3ad1-498b-a5b1-231c5ef2ebf7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
