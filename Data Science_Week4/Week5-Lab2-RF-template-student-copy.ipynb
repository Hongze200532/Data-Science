{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week5 Lab2 WorkSheet: Random Forest\n",
    "\n",
    "\n",
    "**This lab**: the aim of this lab is to recap/learn:\n",
    "- the core concepts of decision trees\n",
    "- the core concepts of bagging\n",
    "- the core concepts of random forests\n",
    "\n",
    "For further information on the random forest method in sklearn used in this part, refer to: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Generate and plot a dataset\n",
    "\n",
    "First generate and plot a two-dimensional dataset using `make_blobs` function in **sklearn**, with four classes of data points and plot it with different classes in different colours. \n",
    "\n",
    "Note that both `make_blobs` and make_classification create multiclass datasets by allocating each class one or more normally-distributed clusters of points. For more information on generating datasets using **sklearn** functions, refer to: https://scikit-learn.org/stable/datasets/sample_generators.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and visualise a decision tree classifier\n",
    "\n",
    "Fit a simple decision tree classifier to this dataset, which will iteratively split the data along one or the other **axis** using a cut-off value as a quantitative criterion, and at each level assign the label of the new region according to a majority vote of data points within it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function for visualising the output of a classifier\n",
    "\n",
    "Define a new function named `visualize_classifier(model, X, y, ax=None, cmap='rainbow')`, which will take as input a model, X, y, a colour map, fits the model to the dataset, and draw the decision boundary. \n",
    "\n",
    "**Once defined, this is a very useful function that can be used for visualising the output of a classifier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Decision Trees: Visualise the output of a decision tree to its maximum depth\n",
    "\n",
    "Take the decision tree as the classifier (e.g., the model) and apply it to the data set created above, and use the `visualize_classifier(model, X, y, ax=None, cmap='rainbow')` function defined above to visualise a decision tree with no depth being specified explicitly. Observe the display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the output of a decision tree to different depths\n",
    "\n",
    "Now, visualize each tree by a decision tree classifier fitted on this dataset to each of the first four depths, e.g., depth = 1, 2, 3, 4 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the decisiosn tree as a TREE as in Week3 Lab2\n",
    "\n",
    "Use the tree.plot.tree commend as that in Weeks Lab2 and visualize a tree of the depth of your choice (e.g., 2,3 4) and compare the TREE visualization with the scatter plot visualization above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Ensembles of  Decison Tree classifiers via Bagging\n",
    "\n",
    "Use the function `BaggingClassifier(model, n_estimators=??, max_samples=0.8, random_state=1)` to an ensemble of the decision trees. Note `n_estimators` is a hyperparameter which specifies how many individual classifiers shall be included in this ensemble. \n",
    "\n",
    "Observe and compare the output from this ensemble and that from the previous (no-depth limit) decision tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Use the `RandomForestClassifier()` class in **Scikit-Learn** to generate a Random Forest classifier. Observe and compare this output with any of the previous outputs that are relevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning in bagging classifiers and random forest classifiers\n",
    "\n",
    "There are parameters in these classifiers that need to be tuned. When creating bagging classifiers the main parameters are `n_estimators` and `max_samples`. When creating random forests the main parameters are `n_estimators`, `max_features` and `max_samples`. \n",
    "\n",
    "You can experiment on varying these parameters and observe the outcomes of different classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  A Random Forest for Classifying Digits\n",
    "\n",
    "### Load a toy dataset\n",
    "\n",
    "Load the **digits** toy dataset from sklearn. scikit-learn comes with a few small standard datasets that do not require to download any file from some external website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_digits\n",
    "# digits = load_digits()\n",
    "# digits.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the first few data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the digits using a random forest\n",
    "\n",
    "You can use function `cross_val_score()` to show a performance metrics. \n",
    "\n",
    "**You can select a different number of estimators and a different number of iterations in cross validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use different performance metrics\n",
    "\n",
    "Calculate and print out a different performance measure instead of cross_val_score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
